{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f79e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FPS: 23.803304687336972\n",
      "✅ Done! Extracted 207 frames at 5 fps.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# --- Input video path ---\n",
    "video_path = \"fingering__expt_video (1).mp4\"\n",
    "\n",
    "# --- Output directory for frames ---\n",
    "output_dir = \"frames_5fps\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Open the video ---\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get original FPS of video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Original FPS: {fps}\")\n",
    "\n",
    "# Define extraction rate (5 fps)\n",
    "target_fps = 5\n",
    "frame_interval = int(fps /target_fps)  # pick every Nth frame\n",
    "\n",
    "frame_count = 0\n",
    "saved_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Save every Nth frame\n",
    "    if frame_count % frame_interval == 0:\n",
    "        frame_name = os.path.join(output_dir, f\"frame_{saved_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_name, frame)\n",
    "        saved_count += 1\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "print(f\"✅ Done! Extracted {saved_count} frames at {target_fps} fps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9160416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LARGER preprocessing (512x512) for 207 frames...\n",
      "\n",
      "Large-format preprocessing complete!\n",
      "All processed color frames have been saved as 512x512 images in the 'output' folder.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the folder containing the original frames\n",
    "input_folder = 'frames_5fps'\n",
    "\n",
    "# Path to the folder where processed frames will be saved\n",
    "output_folder = 'output'\n",
    "\n",
    "# --- ADJUSTED OUTPUT SIZE ---\n",
    "# Increased the final dimensions to 512x512 for a larger, more detailed image.\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "\n",
    "# --- Preprocessing Script ---\n",
    "\n",
    "# 1. Create the output directory if it doesn't already exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"Created directory: {output_folder}\")\n",
    "\n",
    "# 2. Get a sorted list of all frame filenames from the input folder\n",
    "try:\n",
    "    files = sorted(os.listdir(input_folder))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The input folder '{input_folder}' was not found.\")\n",
    "    print(\"Please make sure this script is in the same directory as your frames folder.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Starting LARGER preprocessing ({IMG_WIDTH}x{IMG_HEIGHT}) for {len(files)} frames...\")\n",
    "\n",
    "# 3. Loop through each file in the directory\n",
    "for filename in files:\n",
    "    # Ensure we're only processing image files\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        # Construct the full path to the input image\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Read the image from the file in color\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not read image {filename}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- Preprocessing Steps ---\n",
    "\n",
    "        # a) Crop the image to capture the full circular dish\n",
    "        height, width, _ = img.shape\n",
    "        center_x, center_y = width // 2, height // 2\n",
    "        crop_size = min(height, width, 950) \n",
    "        \n",
    "        start_x = max(center_x - crop_size // 2, 0)\n",
    "        end_x = min(center_x + crop_size // 2, width)\n",
    "        start_y = max(center_y - crop_size // 2, 0)\n",
    "        end_y = min(center_y + crop_size // 2, height)\n",
    "        \n",
    "        cropped_img = img[start_y:end_y, start_x:end_x]\n",
    "\n",
    "        # b) Apply Denoising on the color image\n",
    "        denoised_img = cv2.medianBlur(cropped_img, 5)\n",
    "\n",
    "        # c) Resize to the new, larger standard dimensions\n",
    "        resized_img = cv2.resize(denoised_img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "        # d) Save the final processed color image to the output folder\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(output_path, resized_img)\n",
    "\n",
    "print(\"\\nLarge-format preprocessing complete!\")\n",
    "print(f\"All processed color frames have been saved as {IMG_WIDTH}x{IMG_HEIGHT} images in the '{output_folder}' folder.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcfae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28561105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All masks saved in 'masks/' and segmented frames in 'segmented/'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Input and output directories\n",
    "input_folder = \"output\"\n",
    "mask_folder = \"masks\"\n",
    "seg_folder = \"segmented\"\n",
    "\n",
    "# Create folders if they don’t exist\n",
    "os.makedirs(mask_folder, exist_ok=True)\n",
    "os.makedirs(seg_folder, exist_ok=True)\n",
    "\n",
    "for file_name in sorted(os.listdir(input_folder)):\n",
    "    if file_name.endswith(\".jpg\") or file_name.endswith(\".png\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "        # Read image\n",
    "        img = cv2.imread(file_path)\n",
    "\n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Define HSV range for reddish/pink fingers\n",
    "        lower = np.array([130, 50, 50])   # adjust if needed\n",
    "        upper = np.array([179, 255, 255]) # upper red/pink\n",
    "\n",
    "        # Create mask\n",
    "        mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "        # Clean mask\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, kernel, iterations=1)\n",
    "\n",
    "        # Apply mask\n",
    "        segmented = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "        # Save mask (binary image)\n",
    "        mask_path = os.path.join(mask_folder, f\"mask_{file_name}\")\n",
    "        cv2.imwrite(mask_path, mask)\n",
    "\n",
    "        # Save segmented image\n",
    "        seg_path = os.path.join(seg_folder, f\"seg_{file_name}\")\n",
    "        cv2.imwrite(seg_path, segmented)\n",
    "\n",
    "print(\"✅ All masks saved in 'masks/' and segmented frames in 'segmented/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d661716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_14500\\3749980569.py:197: UserWarning: Using a target size (torch.Size([8, 1, 3, 128, 128])) that is different to the input size (torch.Size([1, 8, 3, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  l1 = F.l1_loss(yhat, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss 0.6032 | SSIM 0.1417 | PSNR 7.50 | IoU 0.000 | Dice 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 207\u001b[39m\n\u001b[32m    205\u001b[39m best_ssim = -\u001b[32m1\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     tr_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     model.eval()\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 200\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    198\u001b[39m ssim_loss = \u001b[32m1.0\u001b[39m - torch.mean(torch.stack([ssim(yhat[\u001b[32m0\u001b[39m][i], y[\u001b[32m0\u001b[39m][i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y.shape[\u001b[32m1\u001b[39m])]))\n\u001b[32m    199\u001b[39m loss = \u001b[32m0.7\u001b[39m*l1 + \u001b[32m0.3\u001b[39m*ssim_loss\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m opt.step()\n\u001b[32m    202\u001b[39m total += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# PredRNN-lite Training Script\n",
    "# ==============================\n",
    "\n",
    "import os, glob, numpy as np\n",
    "from PIL import Image\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------\n",
    "DATA_ROOT = \"segmented\" # <-- change to \"masks\" if needed\n",
    "IMG_SIZE = 128\n",
    "SEQ_IN = 8\n",
    "PRED_STEPS = 1\n",
    "TRAIN_FRAC = 0.8\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-3\n",
    "SAVE_DIR = \"predrnn_outputs\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------\n",
    "# UTILITIES\n",
    "# ------------------------------\n",
    "def list_images(folder):\n",
    "    exts = (\"*.png\",\"*.jpg\",\"*.jpeg\",\"*.bmp\",\"*.tif\",\"*.tiff\")\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files += glob.glob(os.path.join(folder, e))\n",
    "    return sorted(files)\n",
    "\n",
    "def to_tensor(img):\n",
    "    arr = np.asarray(img).astype(np.float32) / 255.0\n",
    "    if arr.ndim == 2:\n",
    "        arr = arr[..., None]\n",
    "    arr = np.transpose(arr, (2,0,1))  # HWC -> CHW\n",
    "    return torch.from_numpy(arr)\n",
    "\n",
    "def psnr(pred, target, eps=1e-8):\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse.item() == 0:\n",
    "        return torch.tensor(99.0)\n",
    "    return 20.0 * torch.log10(1.0 / torch.sqrt(mse + eps))\n",
    "\n",
    "def ssim(pred, target, C1=0.01**2, C2=0.03**2):\n",
    "    mu_x = pred.mean(dim=(-2,-1), keepdim=True)\n",
    "    mu_y = target.mean(dim=(-2,-1), keepdim=True)\n",
    "    sigma_x = ((pred - mu_x)**2).mean(dim=(-2,-1), keepdim=True)\n",
    "    sigma_y = ((target - mu_y)**2).mean(dim=(-2,-1), keepdim=True)\n",
    "    sigma_xy = ((pred - mu_x)*(target - mu_y)).mean(dim=(-2,-1), keepdim=True)\n",
    "    num = (2*mu_x*mu_y + C1) * (2*sigma_xy + C2)\n",
    "    den = (mu_x**2 + mu_y**2 + C1) * (sigma_x + sigma_y + C2)\n",
    "    return (num / (den + 1e-8)).mean()\n",
    "\n",
    "def binarize(x, thresh=0.5):\n",
    "    return (x >= thresh).float()\n",
    "\n",
    "def iou_and_dice(pred, target):\n",
    "    inter = (pred * target).sum()\n",
    "    union = ((pred + target) > 0).float().sum()\n",
    "    iou = (inter / (union + 1e-8)).item()\n",
    "    dice = (2*inter / (pred.sum()+target.sum()+1e-8)).item()\n",
    "    return iou, dice\n",
    "\n",
    "# ------------------------------\n",
    "# DATASET\n",
    "# ------------------------------\n",
    "class FrameSeqDataset(Dataset):\n",
    "    def __init__(self, folder, img_size=128, seq_in=8, pred_steps=1, split=\"train\", train_frac=0.8):\n",
    "        files = list_images(folder)\n",
    "        if len(files) < (seq_in + pred_steps + 1):\n",
    "            raise RuntimeError(\"Not enough frames found.\")\n",
    "\n",
    "        # split by time\n",
    "        n = len(files)\n",
    "        split_idx = int(n * train_frac)\n",
    "        if split == \"train\":\n",
    "            files = files[:split_idx]\n",
    "        else:\n",
    "            files = files[split_idx:]\n",
    "\n",
    "        self.seq_in, self.pred_steps, self.size = seq_in, pred_steps, img_size\n",
    "        self.files = files\n",
    "        self.is_gray = Image.open(files[0]).convert(\"RGB\").mode != \"RGB\"  # auto-detect\n",
    "\n",
    "        # windows\n",
    "        self.windows = []\n",
    "        for i in range(0, len(files) - (seq_in + pred_steps) + 1):\n",
    "            idx = list(range(i, i + seq_in + pred_steps))\n",
    "            self.windows.append(idx)\n",
    "\n",
    "    def __len__(self): return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idxs = self.windows[i]\n",
    "        imgs = []\n",
    "        for j in idxs:\n",
    "            img = Image.open(self.files[j])\n",
    "            if img.mode != \"RGB\":\n",
    "                img = img.convert(\"L\")\n",
    "            else:\n",
    "                img = img.convert(\"RGB\")\n",
    "            img = img.resize((self.size, self.size), Image.BILINEAR)\n",
    "            imgs.append(to_tensor(img))\n",
    "        imgs = torch.stack(imgs, dim=0)  # T,C,H,W\n",
    "        return imgs[:self.seq_in], imgs[self.seq_in:self.seq_in+self.pred_steps]\n",
    "\n",
    "# ------------------------------\n",
    "# MODEL\n",
    "# ------------------------------\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels + hidden_channels, 4*hidden_channels, kernel_size, padding=padding)\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        gates = self.conv(combined)\n",
    "        i, f, o, g = torch.chunk(gates, 4, dim=1)\n",
    "        i = torch.sigmoid(i); f = torch.sigmoid(f); o = torch.sigmoid(o); g = torch.tanh(g)\n",
    "        c_next = f*c + i*g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, in_ch, hidden_chs=[64,64], kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        chs = [in_ch] + hidden_chs\n",
    "        for i in range(len(hidden_chs)):\n",
    "            self.layers.append(ConvLSTMCell(chs[i], hidden_chs[i], kernel_size))\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        B, C, H, W = x_seq.shape[1:]\n",
    "        hs = [torch.zeros(B, cell.hidden_channels, H, W, device=x_seq.device) for cell in self.layers]\n",
    "        cs = [torch.zeros_like(h) for h in hs]\n",
    "        outputs = []\n",
    "        for t in range(x_seq.shape[0]):\n",
    "            inp = x_seq[t]\n",
    "            for li, cell in enumerate(self.layers):\n",
    "                h, c = hs[li], cs[li]\n",
    "                h, c = cell(inp, h, c)\n",
    "                hs[li], cs[li] = h, c\n",
    "                inp = h\n",
    "            outputs.append(inp)\n",
    "        return outputs\n",
    "\n",
    "class PredRNNLite(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.enc = ConvLSTM(in_ch, hidden_chs=[64,64])\n",
    "        self.dec = ConvLSTM(in_ch, hidden_chs=[64,64])\n",
    "        self.readout = nn.Conv2d(64, in_ch, 1)\n",
    "\n",
    "    def forward(self, x_seq, pred_steps=1):\n",
    "        enc_outs = self.enc(x_seq)\n",
    "        prev = x_seq[-1]\n",
    "        outs = []\n",
    "        for _ in range(pred_steps):\n",
    "            dec_in = torch.stack([prev], dim=0)\n",
    "            dec_outs = self.dec(dec_in)\n",
    "            y = torch.sigmoid(self.readout(dec_outs[-1]))\n",
    "            outs.append(y)\n",
    "            prev = y\n",
    "        return torch.stack(outs, dim=0)\n",
    "\n",
    "# ------------------------------\n",
    "# TRAINING\n",
    "# ------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# auto-detect channels\n",
    "test_img = Image.open(list_images(DATA_ROOT)[0])\n",
    "C_in = 1 if test_img.mode != \"RGB\" else 3\n",
    "\n",
    "dtrain = FrameSeqDataset(DATA_ROOT, IMG_SIZE, SEQ_IN, PRED_STEPS, split=\"train\", train_frac=TRAIN_FRAC)\n",
    "dtest  = FrameSeqDataset(DATA_ROOT, IMG_SIZE, SEQ_IN, PRED_STEPS, split=\"test\",  train_frac=TRAIN_FRAC)\n",
    "train_loader = DataLoader(dtrain, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(dtest,  batch_size=1, shuffle=False)\n",
    "\n",
    "model = PredRNNLite(in_ch=C_in).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for x,y in train_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(x, pred_steps=PRED_STEPS)\n",
    "        l1 = F.l1_loss(yhat, y)\n",
    "        ssim_loss = 1.0 - torch.mean(torch.stack([ssim(yhat[0][i], y[0][i]) for i in range(y.shape[1])]))\n",
    "        loss = 0.7*l1 + 0.3*ssim_loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    return total/len(train_loader)\n",
    "\n",
    "best_ssim = -1\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss = train_epoch()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ssim_list, psnr_list, iou_list, dice_list = [], [], [], []\n",
    "        for x,y in test_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            yhat = model(x, pred_steps=PRED_STEPS)\n",
    "            yh, yt = yhat[0,0], y[0,0]\n",
    "            s = ssim(yh, yt).item(); p = psnr(yh, yt).item()\n",
    "            ssim_list.append(s); psnr_list.append(p)\n",
    "            # IoU/Dice only if grayscale\n",
    "            yh_g = torch.mean(yh, dim=0, keepdim=True) if C_in==3 else yh\n",
    "            yt_g = torch.mean(yt, dim=0, keepdim=True) if C_in==3 else yt\n",
    "            yh_b, yt_b = binarize(yh_g, 0.5), binarize(yt_g, 0.5)\n",
    "            iou,dice = iou_and_dice(yh_b, yt_b)\n",
    "            iou_list.append(iou); dice_list.append(dice)\n",
    "        m_ssim, m_psnr = np.mean(ssim_list), np.mean(psnr_list)\n",
    "        m_iou, m_dice = np.mean(iou_list), np.mean(dice_list)\n",
    "    if m_ssim > best_ssim:\n",
    "        best_ssim = m_ssim\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_predrnn.pt\"))\n",
    "    print(f\"Epoch {epoch:02d} | loss {tr_loss:.4f} | SSIM {m_ssim:.4f} | PSNR {m_psnr:.2f} | IoU {m_iou:.3f} | Dice {m_dice:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# SAVE SAMPLES\n",
    "# ------------------------------\n",
    "os.makedirs(os.path.join(SAVE_DIR,\"samples\"), exist_ok=True)\n",
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR,\"best_predrnn.pt\"), map_location=device))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for k,(x,y) in enumerate(test_loader):\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        yhat = model(x, pred_steps=PRED_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d079438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
